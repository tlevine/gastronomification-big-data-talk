Good day, thank you to everyone for coming, and thank you to BrightTALK
for organizing such a wonderful business intelligence summit.

I'm Thomas Levine. I've been working at the intersection of data analytics
and usability for the past 25 years, and I'm currently the director of product
at CSV. And as you might guess, I have some pretty strong views on data visualization.

(slide)

At CSV, we're *frustrated* with data visualization. Data visualization was
was fine 20 years ago, but it can't handle the enormous scales of big data
that we have today.

(slide)

data, visualize, eyes, &c.

(slide)

Data visualization doesn't have enough bandwidth.

(slide)

To illustrate that, let's take a look at this data table.
These data are about rental apartments. Each horizontal row
in the table is an apartment. The "rowid" is the identification
number, the "price" is the monthly rental price, and the
"updated" is the date at which the information was updated.
We're missing a lot in the "updated" column.

I want you to tell me how many dimensions, or variables, this data table has.

(slide)

You said....

I would say that this data table has three dimensions, one per column.
It's also appropriate to say that the "rowid" column is really an identifier
rather than a dimension.

The point here is that the number of dimensions that a dataset has is
pretty much the same thing as the number of columns that a data table has.

This data table very long and skinny, with only a few dimensions but with
lots of records. That is, it has only a few vertical columns but has lots of horizontal rows.
We call this a "voluminous" spreadsheet; it is very simple, but it has
lots of stuff in it.

(slide)

Compare that to this data table. This one only has five rows, so we only have
data about five apartments. On the other hand, it has many columns, so we have
a lot of information about each apartment. We call this a "complex" spreadsheet.

(slide)

What happens when we have a spreadsheet that is both voluminous and complex?
That's a big spreadsheet. The challenge with today's big data is that it is
both high-volume and high-complexity.

(slide)

It is absolutely impossible for anyone to understand a big dataset. In order
to understand it, we have to reduce its volume, simplify it, and work in a team.

(slide)

Reducing the data volume is relatively easy.
We reduce the data volume with aggregations, or statistics, like means, medians,
counts, and so on. Smart people pick this up pretty quickly.

(slide)

Reducing the complexity is hard. We need to use advanced modeling techniques
to for this, and you have to be careful about choosing the correct model.
There's a lot of new technology that is starting to make this a bit easier,
but you basically need to be a data scientist or statistician in order to do
this well.

(slide)

I propose that we don't need to reduce the complexity. Instead, we just need
to put more dimensions into our plots. Let me show you what I mean by that.
